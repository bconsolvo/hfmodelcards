{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc314db-d8c4-4aa7-af23-76b03995a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea60ba4-abdd-455a-8854-d1a68783c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the PyTorch device\n",
    "if torch.xpu.is_available(): # if using Intel Max GPU\n",
    "    device = torch.device('xpu')\n",
    "else: #else use the Intel Xeon CPU\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b1a068-2fc0-4e6e-96f3-1991a333fec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18194446-66d4-48ba-9ad0-efa140fa4c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17887c01-d4f9-44b8-b286-108cea3be552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install latest transformers\n",
    "#!pip uninstall -y transformers && \n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !source /opt/intel/oneapi/setvars.sh #comment out if not running on Intel Developer Cloud Jupyter\n",
    "# !pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65083926-7c52-4773-b279-7bd431b9f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7956fb9d9a244a469ff3f290582bc5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba082b2c74744ffb3555d6a7a7f27e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc9073027f84faebc67652b4c88f382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05df3ceac0d34c5ea4a38658c6dcfecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a05eca2dd06478398b65f2725583c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd43e0a75b6741299b6b3a8f4b003050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9623056dd2ee429ea616257078e56860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190976ca8818413d80b4763dc71c8b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572398dbc3424852bce34b543af907cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d637dc21f51946778e5ed27c96ccba0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f419578da56946b5bc221a90343b303c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54c294ef93641908e76c5b7c51a5be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#Loading the model and tokenizer and putting it on the correct device\n",
    "model_name = 'Intel/neural-chat-7b-v3-3'\n",
    "model_name = 'microsoft/phi-2'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True).to(device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9131a59a-bf53-42cc-857f-3daacf708819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers_modules.microsoft.phi-2.3f879ff35d20d910f0968d0b9b35a9fc074ecb27.modeling_phi.PhiForCausalLM"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "455591b4-187e-400d-9c5b-055cfa9c0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, \n",
    "                      tokenizer, \n",
    "                      system_input = \"You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer.\",\n",
    "                      user_input = \"calculate 100 + 520 + 60\"):\n",
    "    '''\n",
    "    A function to take a textual prompt, encode it into tokens, and generate new text based on the prompt.\n",
    "\n",
    "    Arguments:\n",
    "        model: The Transformers loaded model\n",
    "        tokenizer: The Transformers tokenizer\n",
    "        system_input (str): A string of English instructions on what the model should do\n",
    "        user_input (str): A follow-up string that is concatenated to the system_input to \n",
    "            ask a more specific question or make a statement to complete by the model.\n",
    "    '''\n",
    "    # Format the input using the provided template\n",
    "    prompt = f\"### System:\\n{system_input}\\n### User:\\n{user_input}\\n### Assistant:\\n\"\n",
    "\n",
    "    # Tokenize and encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Set pad_token if it's not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Generate a response\n",
    "    outputs = model.generate(inputs,\n",
    "                             max_length=1000, \n",
    "                             num_return_sequences=1,\n",
    "                            pad_token_id = tokenizer.pad_token_id)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_assistant_only = response.split(\"### Assistant:\\n\")[-1]\n",
    "    print(response_assistant_only)\n",
    "    # Extract only the assistant's response\n",
    "    return response_assistant_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "042fdf07-484d-4c25-86e1-48513139d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's solve this step by step. First, we need to add 100 and 520. \n",
      "100 + 520 = 620\n",
      "Now, we need to add 620 and 60.\n",
      "620 + 60 = 680\n",
      "Therefore, the answer is 680.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "\n",
    "response = generate_response(model, tokenizer)\n",
    "\n",
    "# expected response\n",
    "# \"\"\"\n",
    "# To calculate the sum of 100, 520, and 60, we will follow these steps:\n",
    "\n",
    "# 1. Add the first two numbers: 100 + 520\n",
    "# 2. Add the result from step 1 to the third number: (100 + 520) + 60\n",
    "\n",
    "# Step 1: Add 100 and 520\n",
    "# 100 + 520 = 620\n",
    "\n",
    "# Step 2: Add the result from step 1 to the third number (60)\n",
    "# (620) + 60 = 680\n",
    "\n",
    "# So, the sum of 100, 520, and 60 is 680.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85de9a9-1390-456c-b86c-42c7c764bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    whatever()\n",
    "except Exception as e:\n",
    "    logging.error(traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
